{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_demo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ojb7_-pHZ5Dj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c45d9cf9-466e-48dd-bca9-acdc72ac83df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytrec_eval\n",
            "  Downloading pytrec_eval-0.5.tar.gz (15 kB)\n",
            "Building wheels for collected packages: pytrec-eval\n",
            "  Building wheel for pytrec-eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytrec-eval: filename=pytrec_eval-0.5-cp37-cp37m-linux_x86_64.whl size=262389 sha256=7fd9bf1acb31288ad7b6e1dfc1f23f1f2e900b8b0a9402c22fa8eb3a45534295\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/96/77/0829b8b2606f90f61ba10a51277629d2b615604e122ee932f4\n",
            "Successfully built pytrec-eval\n",
            "Installing collected packages: pytrec-eval\n",
            "Successfully installed pytrec-eval-0.5\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "    Some handy functions for pytroch model training ...\n",
        "\"\"\"\n",
        "!pip install pytrec_eval\n",
        "import torch\n",
        "import sys\n",
        "# sys.path.insert(1, 'qrec')\n",
        "# from ConversationalMF import *\n",
        "import math\n",
        "import pandas as pd\n",
        "import random\n",
        "import pytrec_eval\n",
        "\n",
        "\n",
        "class Evaluator:\n",
        "    def __init__(self, metrics):\n",
        "        self.result = {}\n",
        "        self.metrics = metrics\n",
        "\n",
        "    def evaluate(self, predict, test):\n",
        "        evaluator = pytrec_eval.RelevanceEvaluator(test, self.metrics)\n",
        "        self.result = evaluator.evaluate(predict)\n",
        "        return self.result\n",
        "\n",
        "    def show(self, metrics):\n",
        "        result = {}\n",
        "        for metric in metrics:\n",
        "            res = pytrec_eval.compute_aggregated_measure(metric, [user[metric] for user in self.result.values()])\n",
        "            result[metric] = res\n",
        "            # print('{}={}'.format(metric, res))\n",
        "        return result\n",
        "\n",
        "    def show_all(self):\n",
        "        key = next(iter(self.result.keys()))\n",
        "        keys = self.result[key].keys()\n",
        "        return self.show(keys)\n",
        "\n",
        "\n",
        "def get_evaluations_final(run_mf, test):\n",
        "    metrics = {'recall_5', 'recall_10', 'recall_20', 'P_5', 'P_10', 'P_20', 'map_cut_10','ndcg_cut_10'}\n",
        "    eval_obj = Evaluator(metrics)\n",
        "    indiv_res = eval_obj.evaluate(run_mf, test)\n",
        "    overall_res = eval_obj.show_all()\n",
        "    return overall_res, indiv_res\n",
        "    \n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "#     np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "# Checkpoints\n",
        "def save_checkpoint(model, model_dir):\n",
        "    torch.save(model.state_dict(), model_dir)\n",
        "\n",
        "\n",
        "def resume_checkpoint(model, model_dir, device_id, maml_bool=False):\n",
        "    state_dict = torch.load(model_dir,\n",
        "                        map_location=lambda storage, loc: storage.cuda(device=device_id))  # ensure all storage are on gpu\n",
        "    \n",
        "    if maml_bool:\n",
        "        for key in list(state_dict.keys()):\n",
        "            new_key = key.replace('module.', '')\n",
        "            state_dict[new_key] = state_dict[key]\n",
        "            del state_dict[key]\n",
        "    \n",
        "    model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "\n",
        "def use_cuda(enabled, device_id=0):\n",
        "    if enabled:\n",
        "        assert torch.cuda.is_available(), 'CUDA is not available'\n",
        "        torch.cuda.set_device(device_id)\n",
        "\n",
        "\n",
        "def use_optimizer(network, params):\n",
        "    if params['optimizer'] == 'sgd':\n",
        "        optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, network.parameters()),\n",
        "                                    lr=params['sgd_lr'],\n",
        "                                    momentum=params['sgd_momentum'],\n",
        "                                    weight_decay=params['l2_regularization'])\n",
        "    elif params['optimizer'] == 'adam':\n",
        "        \n",
        "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, network.parameters()), \n",
        "                                                          lr=params['adam_lr'],\n",
        "                                                          weight_decay=params['l2_regularization'])\n",
        "    elif params['optimizer'] == 'rmsprop':\n",
        "        optimizer = torch.optim.RMSprop(filter(lambda p: p.requires_grad, network.parameters()),\n",
        "                                        lr=params['rmsprop_lr'],\n",
        "                                        alpha=params['rmsprop_alpha'],\n",
        "                                        momentum=params['rmsprop_momentum'])\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_model_cid_dir(args, model_type, flip=False):\n",
        "    \"\"\"\n",
        "    based on args and model type, this function generates idbank and checkpoint file dirs\n",
        "    \"\"\"\n",
        "    src_market = args.aug_src_market\n",
        "    tgt_market = args.tgt_market\n",
        "    if flip:\n",
        "        src_market = args.tgt_market \n",
        "        tgt_market = args.aug_src_market\n",
        "    \n",
        "    \n",
        "    tmp_exp_name = f'{args.data_augment_method}_{args.data_sampling_method}'\n",
        "    tmp_src_markets = src_market\n",
        "    if args.data_augment_method == 'no_aug':\n",
        "        #src_market = 'xx'\n",
        "        tmp_exp_name = f'{args.data_augment_method}'\n",
        "        tmp_src_markets = 'single'\n",
        "    \n",
        "    model_dir = f'checkpoints/{tgt_market}_{model_type}_{tmp_src_markets}_{tmp_exp_name}_{args.exp_name}.model'\n",
        "    cid_dir = f'checkpoints/{tgt_market}_{model_type}_{tmp_src_markets}_{tmp_exp_name}_{args.exp_name}.pickle'\n",
        "    return model_dir, cid_dir\n",
        "\n",
        "\n",
        "def get_model_config(model_type):\n",
        "    \n",
        "    gmf_config = {'alias': 'gmf',\n",
        "                  'adam_lr': 0.005, #1e-3,\n",
        "                  'latent_dim': 8,\n",
        "                  'l2_regularization': 1e-07, #0, # 0.01\n",
        "                  'embedding_user': None,\n",
        "                  'embedding_item': None,\n",
        "                  }\n",
        "\n",
        "    mlp_config = {'alias': 'mlp',\n",
        "                  'adam_lr': 0.01, #1e-3,\n",
        "                  'latent_dim': 8,\n",
        "                  'layers': [16,64,32,16,8],  # layers[0] is the concat of latent user vector & latent item vector\n",
        "                  'l2_regularization': 1e-07, #0.0000001,  # MLP model is sensitive to hyper params\n",
        "                  'pretrain': True,\n",
        "                  'embedding_user': None,\n",
        "                  'embedding_item': None,\n",
        "                 }\n",
        "\n",
        "    neumf_config = {'alias': 'nmf',\n",
        "                    'adam_lr': 0.01, #1e-3,\n",
        "                    'latent_dim_mf': 8,\n",
        "                    'latent_dim_mlp': 8,\n",
        "                    'layers': [16,64,32,16,8],  # layers[0] is the concat of latent user vector & latent item vector\n",
        "                    'l2_regularization': 1e-07, #0.0000001, #0.01,\n",
        "                    'pretrain': True,\n",
        "                    'embedding_user': None,\n",
        "                    'embedding_item': None,\n",
        "                    }\n",
        "    \n",
        "    config = {\n",
        "      'gmf': gmf_config,\n",
        "      'mlp': mlp_config,\n",
        "      'nmf': neumf_config}[model_type]\n",
        "    \n",
        "    return config\n",
        "\n",
        "\n",
        "# conduct the testing on the model\n",
        "def test_model(model, config, test_dataloader, test_qrel):\n",
        "    model.eval()\n",
        "    task_rec_all = []\n",
        "    task_unq_users = set()\n",
        "    for test_batch in test_dataloader:\n",
        "        test_user_ids, test_item_ids, test_targets = test_batch\n",
        "        # _get_rankings function\n",
        "        cur_users = [user.item() for user in test_user_ids]\n",
        "        cur_items = [item.item() for item in test_item_ids]\n",
        "\n",
        "        if config['use_cuda'] is True:\n",
        "            test_user_ids, test_item_ids, test_targets = test_user_ids.cuda(), test_item_ids.cuda(), test_targets.cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            batch_scores = model(test_user_ids, test_item_ids)\n",
        "            if config['use_cuda'] is True:\n",
        "                batch_scores = batch_scores.detach().cpu().numpy()\n",
        "            else:\n",
        "                batch_scores = batch_scores.detach().numpy()\n",
        "\n",
        "        for index in range(len(test_user_ids)):\n",
        "            task_rec_all.append((cur_users[index], cur_items[index], batch_scores[index][0].item()))\n",
        "\n",
        "        task_unq_users = task_unq_users.union(set(cur_users))\n",
        "\n",
        "    task_run_mf = get_run_mf(task_rec_all, task_unq_users)\n",
        "    task_ov, task_ind = get_evaluations_final(task_run_mf, test_qrel)\n",
        "    #metron_ndcg, metron_recall = metron_ndcg_recall(task_run_mf, test_qrel, top_k_thr=10)\n",
        "    return task_ov, task_ind\n",
        "\n",
        "\n",
        "def get_run_mf(rec_list, unq_users):\n",
        "    ranking = {}    \n",
        "    for cuser in unq_users:\n",
        "        user_ratings = [x for x in rec_list if x[0]==cuser]\n",
        "        user_ratings.sort(key=lambda x:x[2], reverse=True)\n",
        "        ranking[cuser] = user_ratings\n",
        "\n",
        "    run_mf = {}\n",
        "    for k, v in ranking.items():\n",
        "        cur_rank = {}\n",
        "        for item in v:\n",
        "            cur_rank[str(item[1])]= 2+item[2]\n",
        "        run_mf[str(k)] = cur_rank\n",
        "    return run_mf\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "# "
      ],
      "metadata": {
        "id": "8rlikIcAL6xx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "msjYBIfLL6IP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import random\n",
        "import pandas as pd\n",
        "from copy import deepcopy\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import resource\n",
        "\n",
        "\n",
        "class Central_ID_Bank(object):\n",
        "    \"\"\"\n",
        "    Central for all cross-market user and items original id and their corrosponding index values\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.user_id_index = {}\n",
        "        self.item_id_index = {}\n",
        "        self.last_user_index = 0\n",
        "        self.last_item_index = 0\n",
        "        \n",
        "    def query_user_index(self, user_id):\n",
        "        if user_id not in self.user_id_index:\n",
        "            self.user_id_index[user_id] = self.last_user_index\n",
        "            self.last_user_index += 1\n",
        "        return self.user_id_index[user_id]\n",
        "    \n",
        "    def query_item_index(self, item_id):\n",
        "        if item_id not in self.item_id_index:\n",
        "            self.item_id_index[item_id] = self.last_item_index\n",
        "            self.last_item_index += 1\n",
        "        return self.item_id_index[item_id]\n",
        "    \n",
        "    def query_user_id(self, user_index):\n",
        "        user_index_id = {v:k for k, v in self.user_id_index.items()}\n",
        "        if user_index in user_index_id:\n",
        "            return user_index_id[user_index]\n",
        "        else:\n",
        "            print(f'USER index {user_index} is not valid!')\n",
        "            return 'xxxxx'\n",
        "        \n",
        "    def query_item_id(self, item_index):\n",
        "        item_index_id = {v:k for k, v in self.item_id_index.items()}\n",
        "        if item_index in item_index_id:\n",
        "            return item_index_id[item_index]\n",
        "        else:\n",
        "            print(f'ITEM index {item_index} is not valid!')\n",
        "            return 'yyyyy'\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "class MetaMarket_DataLoader(object):\n",
        "    \"\"\"Data Loader for a few markets, samples task and returns the dataloader for that market\"\"\"\n",
        "    \n",
        "    def __init__(self, task_list, sample_batch_size, task_batch_size=2, shuffle=True, num_workers=0, collate_fn=None,\n",
        "                 pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None):\n",
        "        \n",
        "        self.num_tasks = len(task_list)\n",
        "        self.task_list = task_list\n",
        "        self.shuffle = shuffle\n",
        "        self.num_workers = num_workers\n",
        "        self.sample_batch_size = sample_batch_size\n",
        "        self.task_list_loaders = {\n",
        "            idx:DataLoader(task_list[idx], batch_size=sample_batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=pin_memory) \\\n",
        "            for idx in range(len(self.task_list))\n",
        "        }\n",
        "        self.task_list_iters = {\n",
        "            idx:iter(self.task_list_loaders[idx]) \\\n",
        "            for idx in range(len(self.task_list))\n",
        "        }\n",
        "        self.task_batch_size = min(task_batch_size, self.num_tasks)\n",
        "    \n",
        "    def refresh_dataloaders(self):\n",
        "        self.task_list_loaders = {\n",
        "            idx:DataLoader(self.task_list[idx], batch_size=self.sample_batch_size, shuffle=self.shuffle, num_workers=self.num_workers, pin_memory=False) \\\n",
        "            for idx in range(len(self.task_list))\n",
        "        }\n",
        "        self.task_list_iters = {\n",
        "            idx:iter(self.task_list_loaders[idx]) \\\n",
        "            for idx in range(len(self.task_list))\n",
        "        }\n",
        "        \n",
        "    def get_iterator(self, index):\n",
        "        return self.task_list_iters[index]\n",
        "        \n",
        "    def sample_task(self):\n",
        "        sampled_task_idx = random.randint(0, self.num_tasks-1)\n",
        "        return self.task_list_loaders[sampled_task_idx]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.num_tasks\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.task_list_loaders[index]\n",
        "        \n",
        "        \n",
        "class MetaMarket_Dataset(object):\n",
        "    \"\"\"\n",
        "    Wrapper around market data (task)\n",
        "    ratings: {\n",
        "      0: us_market_gen,\n",
        "      1: de_market_gen,\n",
        "      ...\n",
        "    }\n",
        "    \"\"\"\n",
        "    def __init__(self, task_gen_dict, num_negatives=4, meta_split='train'):\n",
        "        self.num_tasks = len(task_gen_dict)\n",
        "        if meta_split=='train':\n",
        "            self.task_gen_dict = {idx:cur_task.instance_a_market_train_task(idx, num_negatives) for idx, cur_task  in task_gen_dict.items()}\n",
        "        else:\n",
        "            self.task_gen_dict = {idx:cur_task.instance_a_market_valid_task(idx, split=meta_split) for idx, cur_task  in task_gen_dict.items()}\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.num_tasks\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.task_gen_dict[index]\n",
        "    \n",
        "\n",
        "    \n",
        "class SingleMarket_Dataset(object):\n",
        "    \"\"\"\n",
        "    Wrapper around a single pytorch Dataset object\n",
        "    \"\"\"\n",
        "    def __init__(self, mydataset):\n",
        "        self.num_tasks = 1\n",
        "        self.task_gen_dict = {\n",
        "            0: mydataset\n",
        "        }\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.num_tasks\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.task_gen_dict[index]\n",
        "        \n",
        "\n",
        "\n",
        "class MarketTask(Dataset):\n",
        "    \"\"\"\n",
        "    Individual Market data that is going to be wrapped into a metadataset  i.e. MetaMarketDataset\n",
        "\n",
        "    Wrapper, convert <user, item, rate> Tensor into Pytorch Dataset\n",
        "    \"\"\"\n",
        "    def __init__(self, task_index, user_tensor, item_tensor, target_tensor):\n",
        "        \"\"\"\n",
        "        args:\n",
        "\n",
        "            target_tensor: torch.Tensor, the corresponding rating for <user, item> pair\n",
        "        \"\"\"\n",
        "        self.task_index = task_index\n",
        "        self.user_tensor = user_tensor\n",
        "        self.item_tensor = item_tensor\n",
        "        self.target_tensor = target_tensor\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.user_tensor.size(0)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.user_tensor[index], self.item_tensor[index], self.target_tensor[index]\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "class MAML_TaskGenerator(object):\n",
        "    \"\"\"Construct torch dataset\"\"\"\n",
        "    \n",
        "    def __init__(self, ratings, id_index_bank, item_thr=0, users_allow=None, items_allow=None, sample_df=1):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            ratings: pd.DataFrame, which contains 3 columns = ['userId', 'itemId', 'rate']\n",
        "           \n",
        "        \"\"\"\n",
        "        self.ratings = ratings\n",
        "        self.id_index_bank = id_index_bank\n",
        "        \n",
        "        self.item_thr = item_thr\n",
        "        self.sample_df = sample_df\n",
        "        \n",
        "        # filter non_allowed users and items\n",
        "        if users_allow is not None:\n",
        "            self.ratings = self.ratings[self.ratings['userId'].isin( users_allow )]\n",
        "        if items_allow is not None:\n",
        "            self.ratings = self.ratings[self.ratings['itemId'].isin( items_allow )]\n",
        "        \n",
        "        # get item and user pools\n",
        "        self.user_pool_ids = set(self.ratings['userId'].unique())\n",
        "        self.item_pool_ids = set(self.ratings['itemId'].unique())\n",
        "        \n",
        "        # replace ids with corrosponding index for both users and items\n",
        "        self.ratings['userId'] = self.ratings['userId'].apply(lambda x: self.id_index_bank.query_user_index(x) )\n",
        "        self.ratings['itemId'] = self.ratings['itemId'].apply(lambda x: self.id_index_bank.query_item_index(x) )\n",
        "        \n",
        "        # get item and user pools (indexed version)\n",
        "        self.user_pool = set(self.ratings['userId'].unique())\n",
        "        self.item_pool = set(self.ratings['itemId'].unique())\n",
        "        \n",
        "        # specify the splits of the data, normalize the vote\n",
        "        self.user_stats = self._specify_splits()\n",
        "        self.ratings['rate'] = [self.single_vote_normalize(cvote) for cvote in list(self.ratings.rate)]\n",
        "        \n",
        "        # create negative item samples\n",
        "        self.negatives_train, self.negatives_valid, self.negatives_test = self._sample_negative( self.ratings )\n",
        "        \n",
        "        # split the data into train, valid, and test\n",
        "        self.train_ratings, self.valid_ratings, self.test_ratings = self._split_loo( self.ratings )\n",
        "        \n",
        "        \n",
        "    # returns how many training interation for each user has been used \n",
        "    def get_user_stats(self):\n",
        "        return self.user_stats\n",
        "    \n",
        "    \n",
        "    # adds a new column with each split, and removes the rows below the number of item_thr\n",
        "    def _specify_splits(self):\n",
        "        self.ratings = self.ratings.sort_values(['date'],ascending=True)\n",
        "        self.ratings.reset_index(drop=True, inplace=True)\n",
        "        by_userid_group = self.ratings.groupby(\"userId\")\n",
        "        \n",
        "        splits = ['remove'] * len(self.ratings)\n",
        "        \n",
        "        user_stats = {}\n",
        "\n",
        "        for usrid, indice in by_userid_group.groups.items():\n",
        "            cur_item_list = list(indice)\n",
        "            if len(cur_item_list)>= self.item_thr:\n",
        "                train_up_indx = len(cur_item_list)-2\n",
        "                valid_up_index = len(cur_item_list)-1\n",
        "                \n",
        "                sampled_train_up_indx = int(train_up_indx/self.sample_df)\n",
        "        \n",
        "                user_stats[usrid] = len(cur_item_list[:sampled_train_up_indx])\n",
        "\n",
        "                for iind in cur_item_list[:sampled_train_up_indx]:\n",
        "                    splits[iind] = 'train'\n",
        "                for iind in cur_item_list[train_up_indx:valid_up_index]:\n",
        "                    splits[iind] = 'valid'\n",
        "                for iind in cur_item_list[valid_up_index:]:\n",
        "                    splits[iind] = 'test'\n",
        "        self.ratings['split'] = splits\n",
        "        self.ratings = self.ratings[self.ratings['split']!='remove']\n",
        "        self.ratings.reset_index(drop=True, inplace=True)\n",
        "        \n",
        "        return user_stats\n",
        "    \n",
        "    # ratings normalization\n",
        "    def single_vote_normalize(self, cur_vote):\n",
        "        if cur_vote>=1:\n",
        "            return 1.0\n",
        "        else:\n",
        "            return 0.0\n",
        "    \n",
        "    \n",
        "    def _split_loo(self, ratings):\n",
        "        train_sp = ratings[ratings['split']=='train']\n",
        "        valid_sp = ratings[ratings['split']=='valid']\n",
        "        test_sp = ratings[ratings['split']=='test']\n",
        "        return train_sp[['userId', 'itemId', 'rate']], valid_sp[['userId', 'itemId', 'rate']], test_sp[['userId', 'itemId', 'rate']]\n",
        "    \n",
        "    \n",
        "    def _sample_negative(self, ratings):\n",
        "        by_userid_group = self.ratings.groupby(\"userId\")['itemId']\n",
        "        negatives_train = {}\n",
        "        negatives_test = {}\n",
        "        negatives_valid = {}\n",
        "        for userid, group_frame in by_userid_group:\n",
        "            pos_itemids = set(group_frame.values.tolist())\n",
        "            neg_itemids = self.item_pool - pos_itemids\n",
        "            \n",
        "            #neg_itemids_train = random.sample(neg_itemids, min(len(neg_itemids), 1000))\n",
        "            neg_itemids_train = neg_itemids\n",
        "            neg_itemids_test = random.sample(neg_itemids, min(len(neg_itemids), 99))\n",
        "            neg_itemids_valid = random.sample(neg_itemids, min(len(neg_itemids), 99))\n",
        "            \n",
        "            negatives_train[userid] = neg_itemids_train\n",
        "            negatives_test[userid] = neg_itemids_test\n",
        "            negatives_valid[userid] = neg_itemids_valid\n",
        "            \n",
        "        return negatives_train, negatives_valid, negatives_test\n",
        "\n",
        "                                                                    \n",
        "    def instance_a_market_train_task(self, index, num_negatives, data_frac=1):\n",
        "        \"\"\"instance train task's torch Dataset\"\"\"\n",
        "        users, items, ratings = [], [], []\n",
        "        train_ratings = self.train_ratings\n",
        "\n",
        "        for row in train_ratings.itertuples():\n",
        "            users.append(int(row.userId))\n",
        "            items.append(int(row.itemId))\n",
        "            ratings.append(float(row.rate))\n",
        "            \n",
        "            cur_negs = self.negatives_train[int(row.userId)]\n",
        "            cur_negs = random.sample(cur_negs, min(num_negatives, len(cur_negs)) )\n",
        "            for neg in cur_negs:\n",
        "                users.append(int(row.userId))\n",
        "                items.append(int(neg))\n",
        "                ratings.append(float(0))  # negative samples get 0 rating\n",
        "\n",
        "        dataset = MarketTask(index, user_tensor=torch.LongTensor(users),\n",
        "                                        item_tensor=torch.LongTensor(items),\n",
        "                                        target_tensor=torch.FloatTensor(ratings))\n",
        "        return dataset\n",
        "    \n",
        "    \n",
        "    def instance_a_market_train_dataloader(self, index, num_negatives, sample_batch_size, shuffle=True, num_workers=0, data_frac=1):\n",
        "        \"\"\"instance train task's torch Dataloader\"\"\"\n",
        "        dataset = self.instance_a_market_train_task(index, num_negatives, data_frac)\n",
        "        return DataLoader(dataset, batch_size=sample_batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True)\n",
        "        \n",
        "    \n",
        "    def instance_a_market_valid_task(self, index, split='valid'):\n",
        "        \"\"\"instance validation/test task's torch Dataset\"\"\"\n",
        "        cur_ratings = self.valid_ratings\n",
        "        cur_negs = self.negatives_valid\n",
        "        if split.startswith('test'): \n",
        "            cur_ratings = self.test_ratings\n",
        "            cur_negs = self.negatives_test\n",
        "          \n",
        "        users, items, ratings = [], [], []\n",
        "        for row in cur_ratings.itertuples():\n",
        "            users.append(int(row.userId))\n",
        "            items.append(int(row.itemId))\n",
        "            ratings.append(float(row.rate))\n",
        "            \n",
        "            cur_uid_negs = cur_negs[int(row.userId)]\n",
        "            for neg in cur_uid_negs:\n",
        "                users.append(int(row.userId))\n",
        "                items.append(int(neg))\n",
        "                ratings.append(float(0))  # negative samples get 0 rating\n",
        "            \n",
        "        dataset = MarketTask(index, user_tensor=torch.LongTensor(users),\n",
        "                                        item_tensor=torch.LongTensor(items),\n",
        "                                        target_tensor=torch.FloatTensor(ratings))\n",
        "        return dataset\n",
        "    \n",
        "    def instance_a_market_valid_dataloader(self, index, sample_batch_size, shuffle=False, num_workers=0, split='valid'):\n",
        "        \"\"\"instance train task's torch Dataloader\"\"\"\n",
        "        dataset = self.instance_a_market_valid_task(index, split=split)\n",
        "        return DataLoader(dataset, batch_size=sample_batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "\n",
        "    def get_validation_qrel(self, split='valid'):\n",
        "        \"\"\"get pytrec eval version of qrel for evaluation\"\"\"\n",
        "        cur_ratings = self.valid_ratings\n",
        "        if split.startswith('test'): \n",
        "            cur_ratings = self.test_ratings\n",
        "        qrel = {}\n",
        "        for row in cur_ratings.itertuples():\n",
        "            cur_user_qrel = qrel.get(str(row.userId), {})\n",
        "            cur_user_qrel[str(row.itemId)] = int(row.rate)\n",
        "            qrel[str(row.userId)] = cur_user_qrel\n",
        "        return qrel   \n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        " "
      ],
      "metadata": {
        "id": "l52FiWZ3bVFJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class GMF(torch.nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(GMF, self).__init__()\n",
        "        self.num_users = config['num_users']\n",
        "        self.num_items = config['num_items']\n",
        "        self.latent_dim = config['latent_dim']\n",
        "        self.trainable_user = False\n",
        "        self.trainable_item = False\n",
        "\n",
        "        if config['embedding_user'] is None:\n",
        "            self.embedding_user = torch.nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.latent_dim)\n",
        "            self.trainable_user = True\n",
        "        else:\n",
        "            self.embedding_user = config['embedding_user']\n",
        "            \n",
        "        if config['embedding_item'] is None:\n",
        "            self.embedding_item = torch.nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.latent_dim)\n",
        "            self.trainable_item = True\n",
        "        else:\n",
        "            self.embedding_item = config['embedding_item']\n",
        "\n",
        "        self.affine_output = torch.nn.Linear(in_features=self.latent_dim, out_features=1)\n",
        "        self.logistic = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        if self.trainable_user:\n",
        "            user_embedding = self.embedding_user(user_indices)\n",
        "        else:\n",
        "            user_embedding = self.embedding_user[user_indices]\n",
        "        if self.trainable_item:\n",
        "            item_embedding = self.embedding_item(item_indices)\n",
        "        else:\n",
        "            item_embedding = self.embedding_item[item_indices]\n",
        "        element_product = torch.mul(user_embedding, item_embedding)\n",
        "        logits = self.affine_output(element_product)\n",
        "        rating = self.logistic(logits)\n",
        "        return rating\n",
        "\n",
        "    def init_weight(self):\n",
        "        pass\n",
        "    \n",
        "    \n",
        "    \n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(MLP, self).__init__()\n",
        "        self.config = config\n",
        "        self.num_users = config['num_users']\n",
        "        self.num_items = config['num_items']\n",
        "        self.latent_dim = config['latent_dim']\n",
        "\n",
        "        self.embedding_user = torch.nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.latent_dim)\n",
        "        self.embedding_item = torch.nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.latent_dim)\n",
        "\n",
        "        self.fc_layers = torch.nn.ModuleList()\n",
        "        for idx, (in_size, out_size) in enumerate(zip(config['layers'][:-1], config['layers'][1:])):\n",
        "            self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n",
        "\n",
        "        self.affine_output = torch.nn.Linear(in_features=config['layers'][-1], out_features=1)\n",
        "        self.logistic = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        user_embedding = self.embedding_user(user_indices)\n",
        "        item_embedding = self.embedding_item(item_indices)\n",
        "        vector = torch.cat([user_embedding, item_embedding], dim=-1)  # the concat latent vector\n",
        "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
        "            vector = self.fc_layers[idx](vector)\n",
        "            vector = torch.nn.ReLU()(vector)\n",
        "            # vector = torch.nn.BatchNorm1d()(vector)\n",
        "            # vector = torch.nn.Dropout(p=0.5)(vector)\n",
        "        logits = self.affine_output(vector)\n",
        "        rating = self.logistic(logits)\n",
        "        return rating\n",
        "\n",
        "    def init_weight(self):\n",
        "        pass\n",
        "    \n",
        "    def load_pretrain_weights(self, args, maml_bool=False):\n",
        "        \"\"\"Loading weights from trained GMF model\"\"\"\n",
        "        config = self.config\n",
        "        gmf_model = GMF(config)\n",
        "        if config['use_cuda'] is True:\n",
        "            gmf_model.cuda()\n",
        "        gmf_dir, _ = get_model_cid_dir(args, 'gmf')\n",
        "        resume_checkpoint(gmf_model, model_dir = gmf_dir, device_id=config['device_id'], maml_bool=maml_bool)\n",
        "        self.embedding_user.weight.data = gmf_model.embedding_user.weight.data\n",
        "        self.embedding_item.weight.data = gmf_model.embedding_item.weight.data\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "class NeuMF(torch.nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(NeuMF, self).__init__()\n",
        "        self.config = config\n",
        "        self.num_users = config['num_users']\n",
        "        self.num_items = config['num_items']\n",
        "        self.latent_dim_mf = config['latent_dim_mf']\n",
        "        self.latent_dim_mlp = config['latent_dim_mlp']\n",
        "\n",
        "        self.embedding_user_mlp = torch.nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.latent_dim_mlp)\n",
        "        self.embedding_item_mlp = torch.nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.latent_dim_mlp)\n",
        "        self.embedding_user_mf = torch.nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.latent_dim_mf)\n",
        "        self.embedding_item_mf = torch.nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.latent_dim_mf)\n",
        "\n",
        "        self.fc_layers = torch.nn.ModuleList()\n",
        "        for idx, (in_size, out_size) in enumerate(zip(config['layers'][:-1], config['layers'][1:])):\n",
        "            self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n",
        "\n",
        "        self.affine_output = torch.nn.Linear(in_features=config['layers'][-1] + config['latent_dim_mf'], out_features=1)\n",
        "        self.logistic = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        user_embedding_mlp = self.embedding_user_mlp(user_indices)\n",
        "        item_embedding_mlp = self.embedding_item_mlp(item_indices)\n",
        "        user_embedding_mf = self.embedding_user_mf(user_indices)\n",
        "        item_embedding_mf = self.embedding_item_mf(item_indices)\n",
        "\n",
        "        mlp_vector = torch.cat([user_embedding_mlp, item_embedding_mlp], dim=-1)  # the concat latent vector\n",
        "        mf_vector =torch.mul(user_embedding_mf, item_embedding_mf)\n",
        "\n",
        "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
        "            mlp_vector = self.fc_layers[idx](mlp_vector)\n",
        "            mlp_vector = torch.nn.ReLU()(mlp_vector)\n",
        "\n",
        "        vector = torch.cat([mlp_vector, mf_vector], dim=-1)\n",
        "        logits = self.affine_output(vector)\n",
        "        rating = self.logistic(logits)\n",
        "        return rating\n",
        "\n",
        "    def init_weight(self):\n",
        "        pass\n",
        "\n",
        "    def load_pretrain_weights(self, args, maml_bool=False):\n",
        "        \"\"\"Loading weights from trained MLP model & GMF model\"\"\"\n",
        "        config = self.config\n",
        "        config['latent_dim'] = config['latent_dim_mlp']\n",
        "        mlp_model = MLP(config)\n",
        "        if config['use_cuda'] is True:\n",
        "            mlp_model.cuda()\n",
        "        mlp_dir, _ = get_model_cid_dir(args, 'mlp')\n",
        "        resume_checkpoint(mlp_model, model_dir=mlp_dir, device_id=config['device_id'], maml_bool=maml_bool)\n",
        "\n",
        "        self.embedding_user_mlp.weight.data = mlp_model.embedding_user.weight.data\n",
        "        self.embedding_item_mlp.weight.data = mlp_model.embedding_item.weight.data\n",
        "        for idx in range(len(self.fc_layers)):\n",
        "            self.fc_layers[idx].weight.data = mlp_model.fc_layers[idx].weight.data\n",
        "\n",
        "        config['latent_dim'] = config['latent_dim_mf']\n",
        "        gmf_model = GMF(config)\n",
        "        if config['use_cuda'] is True:\n",
        "            gmf_model.cuda()\n",
        "        gmf_dir, _ = get_model_cid_dir(args, 'gmf')\n",
        "        resume_checkpoint(gmf_model, model_dir=gmf_dir, device_id=config['device_id'], maml_bool=maml_bool)\n",
        "        self.embedding_user_mf.weight.data = gmf_model.embedding_user.weight.data\n",
        "        self.embedding_item_mf.weight.data = gmf_model.embedding_item.weight.data\n",
        "\n",
        "        self.affine_output.weight.data = 0.5 * torch.cat([mlp_model.affine_output.weight.data, gmf_model.affine_output.weight.data], dim=-1)\n",
        "        self.affine_output.bias.data = 0.5 * (mlp_model.affine_output.bias.data + gmf_model.affine_output.bias.data)\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "class NeuMF_MH(torch.nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(NeuMF_MH, self).__init__()\n",
        "        self.config = config\n",
        "        self.num_users = config['num_users']\n",
        "        self.num_items = config['num_items']\n",
        "        self.latent_dim_mf = config['latent_dim_mf']\n",
        "        self.latent_dim_mlp = config['latent_dim_mlp']\n",
        "\n",
        "        self.embedding_user_mlp = torch.nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.latent_dim_mlp)\n",
        "        self.embedding_item_mlp = torch.nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.latent_dim_mlp)\n",
        "        self.embedding_user_mf = torch.nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.latent_dim_mf)\n",
        "        self.embedding_item_mf = torch.nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.latent_dim_mf)\n",
        "\n",
        "        self.fc_layers = torch.nn.ModuleList()\n",
        "        for idx, (in_size, out_size) in enumerate(zip(config['layers'][:-1], config['layers'][1:])):\n",
        "            self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n",
        "        \n",
        "        # market head (MH) layers\n",
        "        inout_len = config['layers'][-1] + config['latent_dim_mf']\n",
        "        #mh_layers_dims = [inout_len, 32, inout_len] #[16,64,32,16,8]\n",
        "        #mh_layers_dims = [inout_len, inout_len]\n",
        "        mh_layers_dims = config['mh_layers']\n",
        "        self.mh_layers = torch.nn.ModuleList()\n",
        "        for idx, (in_size, out_size) in enumerate(zip(mh_layers_dims[:-1], mh_layers_dims[1:])):\n",
        "            self.mh_layers.append(torch.nn.Linear(in_size, out_size))\n",
        "        if len(mh_layers_dims)>0:\n",
        "            self.affine_output = torch.nn.Linear(in_features=mh_layers_dims[-1], out_features=1)\n",
        "        else:   \n",
        "            self.affine_output = torch.nn.Linear(in_features=inout_len, out_features=1)\n",
        "        self.logistic = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        user_embedding_mlp = self.embedding_user_mlp(user_indices)\n",
        "        item_embedding_mlp = self.embedding_item_mlp(item_indices)\n",
        "        user_embedding_mf = self.embedding_user_mf(user_indices)\n",
        "        item_embedding_mf = self.embedding_item_mf(item_indices)\n",
        "\n",
        "        mlp_vector = torch.cat([user_embedding_mlp, item_embedding_mlp], dim=-1)  # the concat latent vector\n",
        "        mf_vector =torch.mul(user_embedding_mf, item_embedding_mf)\n",
        "\n",
        "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
        "            mlp_vector = self.fc_layers[idx](mlp_vector)\n",
        "            mlp_vector = torch.nn.ReLU()(mlp_vector)\n",
        "\n",
        "        vector = torch.cat([mlp_vector, mf_vector], dim=-1)\n",
        "        \n",
        "        for idx, _ in enumerate(range(len(self.mh_layers))):\n",
        "            vector = self.mh_layers[idx](vector)\n",
        "            vector = torch.nn.ReLU()(vector)\n",
        "        \n",
        "        logits = self.affine_output(vector)\n",
        "        rating = self.logistic(logits)\n",
        "        return rating\n",
        "\n",
        "    def init_weight(self):\n",
        "        pass\n",
        "\n",
        "    def load_pretrain_weights(self, args, maml_bool=False):\n",
        "        \"\"\"Loading weights from trained MLP model & GMF model\"\"\"\n",
        "        config = self.config\n",
        "        config['latent_dim'] = config['latent_dim_mlp']\n",
        "        mlp_model = MLP(config)\n",
        "        if config['use_cuda'] is True:\n",
        "            mlp_model.cuda()\n",
        "        mlp_dir, _ = get_model_cid_dir(args, 'mlp')\n",
        "        resume_checkpoint(mlp_model, model_dir=mlp_dir, device_id=config['device_id'], maml_bool=maml_bool)\n",
        "\n",
        "        self.embedding_user_mlp.weight.data = mlp_model.embedding_user.weight.data\n",
        "        self.embedding_item_mlp.weight.data = mlp_model.embedding_item.weight.data\n",
        "        for idx in range(len(self.fc_layers)):\n",
        "            self.fc_layers[idx].weight.data = mlp_model.fc_layers[idx].weight.data\n",
        "\n",
        "        config['latent_dim'] = config['latent_dim_mf']\n",
        "        gmf_model = GMF(config)\n",
        "        if config['use_cuda'] is True:\n",
        "            gmf_model.cuda()\n",
        "        gmf_dir, _ = get_model_cid_dir(args, 'gmf')\n",
        "        resume_checkpoint(gmf_model, model_dir=gmf_dir, device_id=config['device_id'], maml_bool=maml_bool)\n",
        "        self.embedding_user_mf.weight.data = gmf_model.embedding_user.weight.data\n",
        "        self.embedding_item_mf.weight.data = gmf_model.embedding_item.weight.data\n",
        "\n",
        "        self.affine_output.weight.data = 0.5 * torch.cat([mlp_model.affine_output.weight.data, gmf_model.affine_output.weight.data], dim=-1)\n",
        "        self.affine_output.bias.data = 0.5 * (mlp_model.affine_output.bias.data + gmf_model.affine_output.bias.data)"
      ],
      "metadata": {
        "id": "AVghlBSEbJzz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This script trains all GMF, MLP, and NMF baselines for a single market\n",
        "Provides three options for the use of another source market:\n",
        "  1. 'no_aug'  : only use the target market train data, hence single market training (the src market will set to 'xx')\n",
        "  2. 'full_aug': fully uses the source market data for training\n",
        "  3. 'sel_aug' : only use portion of source market data covering target market's items\n",
        "  \n",
        "For data sampling:\n",
        "  a. 'equal'   : equally sample data from both source and target markets, providing a balanced training\n",
        "  b. 'concate' : first concatenate the source and target training data, treat that a single training data\n",
        "\"\"\"\n",
        "from google.colab import files\n",
        "import json\n",
        "\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.argv=['']\n",
        "del sys\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
        "\n",
        "# sys.path.insert(1, 'src')\n",
        "# from model import GMF, MLP, NeuMF\n",
        "# from utils import *\n",
        "# from data import *\n",
        "\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import json\n",
        "import resource\n",
        "import sys\n",
        "import pickle\n",
        "\n",
        "\n",
        "def create_arg_parser():\n",
        "    parser = argparse.ArgumentParser('NeuMF_Engine')\n",
        "    # Path Arguments\n",
        "    parser.add_argument('--num_epoch', type=int, default=25, help='number of epoches')\n",
        "    parser.add_argument('--batch_size', type=int, default=1024, help='batch size')\n",
        "    parser.add_argument('--num_neg', type=int, default=4, help='number of negatives to sample during training')\n",
        "    parser.add_argument('--cuda', action='store_true', help='use of cuda')\n",
        "    parser.add_argument('--seed', type=int, default=42, help='manual seed init')\n",
        "    \n",
        "    # output arguments \n",
        "    parser.add_argument('--exp_name', help='name the experiment',type=str, default='exp_name')\n",
        "    parser.add_argument('--exp_output', help='output results .json file',type=str, default='')\n",
        "    \n",
        "    # data arguments \n",
        "    parser.add_argument('--data_dir', help='dataset directory', type=str, default='DATA/')\n",
        "    parser.add_argument('--tgt_market', help='specify target market', type=str, default='de') # de_Electronics\n",
        "    parser.add_argument('--aug_src_market', help='which data to augment with',type=str, default='xx') # us_Electronics\n",
        "    \n",
        "    # augmentation approaches\n",
        "    # aug_method: 'no_aug', 'full_aug', 'sel_aug'\n",
        "    parser.add_argument('--data_augment_method', help='how to augment data to target market',type=str, default='no_aug') \n",
        "    # sampling_method: 'concat'  'equal'\n",
        "    parser.add_argument('--data_sampling_method', help='in augmentation how to sample data for training',type=str, default='concat')\n",
        "    \n",
        "    # MODEL selection\n",
        "    parser.add_argument('--model_selection', help='which nn model to train with', type=str, default='all') # gmf, mlp, nmf\n",
        "    \n",
        "    # cold start setup\n",
        "    parser.add_argument('--tgt_fraction', type=int, default=1, help='what fraction of data to use on target side')\n",
        "    parser.add_argument('--src_fraction', type=int, default=1, help='what fraction of data to use from source side')\n",
        "    \n",
        "     \n",
        "    return parser\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "The main module that takes the model and dataloaders for training and testing on specific target market \n",
        "\"\"\"\n",
        "def train_and_test_model(args, config, model, train_dataloader, valid_dataloader, valid_qrel, test_dataloader, test_qrel):\n",
        "    opt = use_optimizer(model, config)\n",
        "    loss_func = torch.nn.BCELoss()\n",
        "    \n",
        "    ############\n",
        "    ## Train\n",
        "    ############\n",
        "    best_ndcg = 0.0\n",
        "    best_eval_res = {}\n",
        "    all_eval_res = {}\n",
        "    for epoch in range(args.num_epoch):\n",
        "        print('Epoch {} starts !'.format(epoch))\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        # train the model for some certain iterations\n",
        "        train_dataloader.refresh_dataloaders()\n",
        "        #iteration_num = len(train_dataloader[0])\n",
        "        data_lens = [len(train_dataloader[idx]) for idx in range(train_dataloader.num_tasks)]\n",
        "        iteration_num = max(data_lens)\n",
        "        for iteration in range(iteration_num):\n",
        "            for subtask_num in range(train_dataloader.num_tasks): # get one batch from each dataloader\n",
        "                cur_train_dataloader = train_dataloader.get_iterator(subtask_num)\n",
        "                try:\n",
        "                    train_user_ids, train_item_ids, train_targets = next(cur_train_dataloader)\n",
        "                except:\n",
        "                    new_train_iterator = iter(train_dataloader[subtask_num])\n",
        "                    train_user_ids, train_item_ids, train_targets = next(new_train_iterator)\n",
        "                    \n",
        "                if config['use_cuda'] is True:\n",
        "                    train_user_ids, train_item_ids, train_targets = train_user_ids.cuda(), train_item_ids.cuda(), train_targets.cuda()\n",
        "                opt.zero_grad()\n",
        "                ratings_pred = model(train_user_ids, train_item_ids)\n",
        "                loss = loss_func(ratings_pred.view(-1), train_targets)\n",
        "                loss.backward()\n",
        "                opt.step()    \n",
        "                total_loss += loss.item()\n",
        "        sys.stdout.flush()\n",
        "        print('-' * 80)\n",
        "    \n",
        "    ############\n",
        "    ## TEST\n",
        "    ############\n",
        "    #if args.model_selection=='nmf':\n",
        "    valid_ov, valid_ind = test_model(model, config, valid_dataloader, valid_qrel)\n",
        "    cur_ndcg = valid_ov['ndcg_cut_10']\n",
        "    cur_recall = valid_ov['recall_10']\n",
        "    print( f'[pytrec_based] tgt_valid: \\t NDCG@10: {cur_ndcg} \\t R@10: {cur_recall}')\n",
        "\n",
        "    all_eval_res[f'valid'] = {\n",
        "        'agg': valid_ov,\n",
        "        'ind': valid_ind,\n",
        "    }\n",
        "\n",
        "    test_ov, test_ind = test_model(model, config, test_dataloader, test_qrel)\n",
        "    cur_ndcg = test_ov['ndcg_cut_10']\n",
        "    cur_recall = test_ov['recall_10']\n",
        "    print( f'[pytrec_based] tgt_test: \\t NDCG@10: {cur_ndcg} \\t R@10: {cur_recall} \\n\\n')\n",
        "\n",
        "    all_eval_res[f'test'] = {\n",
        "        'agg': test_ov,\n",
        "        'ind': test_ind,\n",
        "    }\n",
        "\n",
        "    return model, all_eval_res \n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = create_arg_parser()\n",
        "    args = parser.parse_args()\n",
        "    set_seed(args)\n",
        "    my_id_bank = Central_ID_Bank()\n",
        "    \n",
        "    ############\n",
        "    ## Target Market data\n",
        "    ############\n",
        "    tgt_data_dir = os.path.join(args.data_dir, f'proc_data/{args.tgt_market}_5core.txt')\n",
        "    print(f'loading {tgt_data_dir}')\n",
        "    tgt_ratings = pd.read_csv(tgt_data_dir, sep=' ')\n",
        "    \n",
        "    tgt_task_generator = MAML_TaskGenerator(tgt_ratings, my_id_bank, item_thr=7, sample_df=args.tgt_fraction)\n",
        "    print('loaded target data!')\n",
        "    \n",
        "    \n",
        "    ############\n",
        "    ## Source Market Data: Augmentation Approaches\n",
        "    ## options: 'no_aug', 'full_aug', or 'sel_aug'\n",
        "    ############\n",
        "    aug_method = args.data_augment_method\n",
        "    if args.aug_src_market=='us':\n",
        "        src_data_dir = os.path.join(args.data_dir, f'proc_data/{args.aug_src_market}_10core.txt')\n",
        "    else:\n",
        "        src_data_dir = os.path.join(args.data_dir, f'proc_data/{args.aug_src_market}_5core.txt')\n",
        "\n",
        "    if aug_method=='no_aug':\n",
        "        src_task_generator = None\n",
        "        args.aug_src_market = 'xx'\n",
        "    if aug_method=='full_aug':\n",
        "        print(f'loading {src_data_dir}')\n",
        "        src_ratings = pd.read_csv(src_data_dir, sep=' ')\n",
        "        src_task_generator = MAML_TaskGenerator(src_ratings, my_id_bank, item_thr=7, sample_df=args.src_fraction)\n",
        "    if aug_method=='sel_aug':\n",
        "        print(f'loading {src_data_dir} with limiting to target data item pool...')\n",
        "        src_ratings = pd.read_csv(src_data_dir, sep=' ')\n",
        "        aug_items_allowed = tgt_task_generator.item_pool_ids\n",
        "        src_task_generator = MAML_TaskGenerator(src_ratings, my_id_bank, item_thr=7, items_allow=aug_items_allowed)\n",
        "\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "    \n",
        "    ############\n",
        "    ## Dataset Concatenation \n",
        "    ## options: 'equal' or 'concat' \n",
        "    ############\n",
        "    print('concatenating target and source data...')\n",
        "    sampling_method = args.data_sampling_method # 'concat'  'equal'\n",
        "\n",
        "    if aug_method=='no_aug':      # 0. only use the target market train data\n",
        "        task_gen_all = {\n",
        "            0: tgt_task_generator,\n",
        "        } \n",
        "        train_tasksets = MetaMarket_Dataset(task_gen_all, num_negatives=args.num_neg, meta_split='train' )\n",
        "        train_dataloader = MetaMarket_DataLoader(train_tasksets, sample_batch_size=args.batch_size, shuffle=True, num_workers=0)\n",
        "\n",
        "    elif sampling_method=='equal': # 1. equally sample from source and target \n",
        "        task_gen_all = {\n",
        "            0: tgt_task_generator,\n",
        "            1: src_task_generator\n",
        "        } \n",
        "        train_tasksets = MetaMarket_Dataset(task_gen_all, num_negatives=args.num_neg, meta_split='train' )\n",
        "        train_dataloader = MetaMarket_DataLoader(train_tasksets, sample_batch_size=args.batch_size, shuffle=True, num_workers=0)\n",
        "\n",
        "    else:                         # 2. concatenate first, and then sample \n",
        "        tgt_task_dataset = tgt_task_generator.instance_a_market_train_task(0, num_negatives=args.num_neg)\n",
        "        src_task_dataset = src_task_generator.instance_a_market_train_task(0, num_negatives=args.num_neg)\n",
        "        train_tasksets = SingleMarket_Dataset( ConcatDataset( [tgt_task_dataset, src_task_dataset]) )\n",
        "        train_dataloader = MetaMarket_DataLoader(train_tasksets, sample_batch_size=args.batch_size, shuffle=True, num_workers=0)\n",
        "\n",
        "    \n",
        "    sys.stdout.flush()\n",
        "\n",
        "    print('preparing test/valid data...')\n",
        "    tgt_user_stats = tgt_task_generator.get_user_stats()\n",
        "    \n",
        "    tgt_valid_dataloader = tgt_task_generator.instance_a_market_valid_dataloader(0, sample_batch_size=args.batch_size, shuffle=False, num_workers=0, split='valid')\n",
        "    tgt_valid_qrel = tgt_task_generator.get_validation_qrel(split='valid')\n",
        "    \n",
        "    tgt_test_dataloader = tgt_task_generator.instance_a_market_valid_dataloader(0, sample_batch_size=args.batch_size, shuffle=False, num_workers=0, split='test')\n",
        "    tgt_test_qrel = tgt_task_generator.get_validation_qrel(split='test')\n",
        "    \n",
        "    \n",
        "    ############\n",
        "    ## Model Prepare \n",
        "    ############\n",
        "    all_model_selection = ['gmf', 'mlp', 'nmf']\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for cur_model_selection in all_model_selection:\n",
        "        sys.stdout.flush()\n",
        "        args.model_selection = cur_model_selection\n",
        "        config = get_model_config(args.model_selection)\n",
        "        config['batch_size'] = args.batch_size\n",
        "        config['optimizer'] = 'adam'\n",
        "        config['use_cuda'] = args.cuda\n",
        "        config['device_id'] = 0\n",
        "        config['save_trained'] = True\n",
        "        config['load_pretrained'] = True\n",
        "        config['num_users'] = int(my_id_bank.last_user_index+1)\n",
        "        config['num_items'] = int(my_id_bank.last_item_index+1)\n",
        "\n",
        "        if args.model_selection=='gmf':\n",
        "            print('model is GMF!')\n",
        "            model = GMF(config)\n",
        "        elif args.model_selection=='nmf':\n",
        "            print('model is NeuMF!')\n",
        "            model = NeuMF(config)\n",
        "            if config['load_pretrained']:\n",
        "                print('loading pretrained gmf and mlp...')\n",
        "                model.load_pretrain_weights(args)\n",
        "        else: # default is MLP\n",
        "            print('model is MLP!')\n",
        "            model = MLP(config)\n",
        "            if config['load_pretrained']:\n",
        "                print('loading pretrained gmf...')\n",
        "                model.load_pretrain_weights(args)\n",
        "\n",
        "        if config['use_cuda'] is True:\n",
        "            use_cuda(True, config['device_id'])\n",
        "            model.cuda()\n",
        "        print(model)\n",
        "        sys.stdout.flush()\n",
        "        model, cur_model_results = train_and_test_model(args, config, model, train_dataloader, tgt_valid_dataloader, tgt_valid_qrel, tgt_test_dataloader, tgt_test_qrel)\n",
        "        \n",
        "        #if args.model_selection=='nmf':\n",
        "        results[args.model_selection] = cur_model_results\n",
        "\n",
        "        ############\n",
        "        ## SAVE the model and idbank\n",
        "        ############\n",
        "        if config['save_trained']:\n",
        "            model_dir, cid_filename = get_model_cid_dir(args, args.model_selection)\n",
        "            save_checkpoint(model, model_dir)\n",
        "            with open(cid_filename, 'wb') as centralid_file:\n",
        "                pickle.dump(my_id_bank, centralid_file)\n",
        "    \n",
        "    \n",
        "    # writing the results into a file      \n",
        "    results['args'] = str(args)\n",
        "    results['user_stats'] = tgt_user_stats\n",
        "    with open('final.csv', 'w') as outfile:\n",
        "        json.dump(results, outfile)\n",
        "    \n",
        "    print('Experiment finished success!')\n",
        "    files.download('final.csv')\n",
        "    \n",
        "if __name__==\"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "wrNT5PoXbpMl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6aa6c573-d7f3-4d38-ffd7-e3505299cfb0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading DATA/proc_data/de_5core.txt\n",
            "loaded target data!\n",
            "concatenating target and source data...\n",
            "preparing test/valid data...\n",
            "model is GMF!\n",
            "GMF(\n",
            "  (embedding_user): Embedding(1852, 8)\n",
            "  (embedding_item): Embedding(2180, 8)\n",
            "  (affine_output): Linear(in_features=8, out_features=1, bias=True)\n",
            "  (logistic): Sigmoid()\n",
            ")\n",
            "Epoch 0 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 2 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 3 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 4 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 5 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 6 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 7 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 8 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 9 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 10 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 11 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 12 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 13 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 14 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 15 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 16 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 17 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 18 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 19 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 20 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 21 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 22 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 23 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 24 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[pytrec_based] tgt_valid: \t NDCG@10: 0.21813552287563903 \t R@10: 0.3490005402485143\n",
            "[pytrec_based] tgt_test: \t NDCG@10: 0.15258397792366213 \t R@10: 0.2614802809292274 \n",
            "\n",
            "\n",
            "model is MLP!\n",
            "loading pretrained gmf...\n",
            "MLP(\n",
            "  (embedding_user): Embedding(1852, 8)\n",
            "  (embedding_item): Embedding(2180, 8)\n",
            "  (fc_layers): ModuleList(\n",
            "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
            "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
            "    (3): Linear(in_features=16, out_features=8, bias=True)\n",
            "  )\n",
            "  (affine_output): Linear(in_features=8, out_features=1, bias=True)\n",
            "  (logistic): Sigmoid()\n",
            ")\n",
            "Epoch 0 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 2 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 3 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 4 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 5 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 6 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 7 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 8 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 9 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 10 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 11 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 12 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 13 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 14 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 15 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 16 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 17 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 18 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 19 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 20 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 21 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 22 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 23 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 24 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[pytrec_based] tgt_valid: \t NDCG@10: 0.18819835282765293 \t R@10: 0.37061048082117776\n",
            "[pytrec_based] tgt_test: \t NDCG@10: 0.13987588689597938 \t R@10: 0.28417071853052406 \n",
            "\n",
            "\n",
            "model is NeuMF!\n",
            "loading pretrained gmf and mlp...\n",
            "NeuMF(\n",
            "  (embedding_user_mlp): Embedding(1852, 8)\n",
            "  (embedding_item_mlp): Embedding(2180, 8)\n",
            "  (embedding_user_mf): Embedding(1852, 8)\n",
            "  (embedding_item_mf): Embedding(2180, 8)\n",
            "  (fc_layers): ModuleList(\n",
            "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
            "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
            "    (3): Linear(in_features=16, out_features=8, bias=True)\n",
            "  )\n",
            "  (affine_output): Linear(in_features=16, out_features=1, bias=True)\n",
            "  (logistic): Sigmoid()\n",
            ")\n",
            "Epoch 0 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 2 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 3 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 4 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 5 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 6 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 7 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 8 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 9 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 10 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 11 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 12 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 13 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 14 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 15 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 16 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 17 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 18 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 19 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 20 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 21 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 22 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 23 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 24 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[pytrec_based] tgt_valid: \t NDCG@10: 0.22930393762247334 \t R@10: 0.4165316045380875\n",
            "[pytrec_based] tgt_test: \t NDCG@10: 0.16365306536934424 \t R@10: 0.3160453808752026 \n",
            "\n",
            "\n",
            "Experiment finished success!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ea3c1417-612c-47ba-bad3-b9cbf6d4fbf3\", \"final.csv\", 1633142)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}